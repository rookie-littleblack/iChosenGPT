# LLM è®­ç»ƒã€æ¨ç†çƒ­é—¨æ¡†æ¶èµ„æº

## åŸå§‹é“¾æ¥
[Awesome-LLM-Inference](https://github.com/DefTruth/Awesome-LLM-Inference/tree/main)

## æ¨ç†æ¡†æ¶
|  Date   |                                                                      Title                                                                      |                                       Paper                                       |                                                                        Code                                                                         | Recom |
|:-------:|:-----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------:|:-----:|  
| 2023.03 |              [FlexGen] High-Throughput Generative Inference of Large Language Models  with a Single GPU(@Stanford University etc)               |                   [[pdf]](https://arxiv.org/pdf/2303.06865.pdf)                   |          [[FlexGen]](https://github.com/FMInference/FlexGen) ![](https://img.shields.io/github/stars/FMInference/FlexGen.svg?style=social)          |  â­ï¸   |          
| 2023.05 | [SpecInfer] Accelerating Generative Large Language Model Serving with Speculative Inference and Token Tree Verification(@Peking University etc) |                   [[pdf]](https://arxiv.org/pdf/2305.09781.pdf)                   |    [[FlexFlow]](https://github.com/flexflow/FlexFlow/tree/inference) ![](https://img.shields.io/github/stars/flexflow/FlexFlow.svg?style=social)    |  â­ï¸   |     
| 2023.05 |                        [FastServe] Fast Distributed Inference Serving for Large Language Models(@Peking University etc)                         |                   [[pdf]](https://arxiv.org/pdf/2305.05920.pdf)                   |                                                                         âš ï¸                                                                          |  â­ï¸   |         
| 2023.09 |                 ğŸ”¥[**vLLM**] Efficient Memory Management for Large Language Model Serving with PagedAttention(@UC Berkeley etc)                 |                   [[pdf]](https://arxiv.org/pdf/2309.06180.pdf)                   |             [[vllm]](https://github.com/vllm-project/vllm) ![](https://img.shields.io/github/stars/vllm-project/vllm.svg?style=social)              | â­ï¸â­ï¸  |     
| 2023.09 |                              [StreamingLLM] EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS(@Meta AI etc)                              |                   [[pdf]](https://arxiv.org/pdf/2309.17453.pdf)                   | [[streaming-llm]](https://github.com/mit-han-lab/streaming-llm) ![](https://img.shields.io/github/stars/mit-han-lab/streaming-llm.svg?style=social) |  â­ï¸   |  
| 2023.09 |                 [Medusa] Medusa: Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads(@Tianle Cai etc)                 |                [[blog]](https://sites.google.com/view/medusa-llm)                 |        [[Medusa]](https://github.com/FasterDecoding/Medusa) ![](https://img.shields.io/github/stars/FasterDecoding/Medusa.svg?style=social)         |  â­ï¸   |    
| 2023.10 |                                                ğŸ”¥[**TensorRT-LLM**] NVIDIA TensorRT LLM(@NVIDIA)                                                |                 [[docs]](https://nvidia.github.io/TensorRT-LLM/)                  |       [[TensorRT-LLM]](https://github.com/NVIDIA/TensorRT-LLM) ![](https://img.shields.io/github/stars/NVIDIA/TensorRT-LLM.svg?style=social)        | â­ï¸â­ï¸  |    
| 2023.11 |   ğŸ”¥[**DeepSpeed-FastGen 2x vLLM?**] DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference(@Microsoft)    |                   [[pdf]](https://arxiv.org/pdf/2401.08671.pdf)                   |     [[deepspeed-fastgen]](https://github.com/microsoft/DeepSpeed) ![](https://img.shields.io/github/stars/microsoft/DeepSpeed.svg?style=social)     | â­ï¸â­ï¸  |  
| 2023.12 |               ğŸ”¥[**PETALS**] Distributed Inference and Fine-tuning of Large Language Models Over The Internet(@HSE Univesity etc)               |                   [[pdf]](https://arxiv.org/pdf/2312.08361.pdf)                   |   [[petals]](https://github.com/bigscience-workshop/petals) ![](https://img.shields.io/github/stars/bigscience-workshop/petals.svg?style=social)    | â­ï¸â­ï¸  | 
| 2023.12 |                           [PowerInfer] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU(@SJTU)                           | [[pdf]](https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf) |      [[PowerInfer]](https://github.com/SJTU-IPADS/PowerInfer) ![](https://img.shields.io/github/stars/SJTU-IPADS/PowerInfer.svg?style=social)       |  â­ï¸   |


## è®­ç»ƒæ¡†æ¶
|  Date   |                                                          Title                                                          |                      Paper                       |                                                                      Code                                                                      | Recom |
|:-------:|:-----------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:-----:|
| 2020.05 |          ğŸ”¥[**Megatron-LM**] Training Multi-Billion Parameter Language Models Using Model Parallelism(@NVIDIA)          |  [[pdf]](https://arxiv.org/pdf/1909.08053.pdf)   |      [[Megatron-LM]](https://github.com/NVIDIA/Megatron-LM) ![](https://img.shields.io/github/stars/NVIDIA/Megatron-LM.svg?style=social)       | â­ï¸â­ï¸  |
| 2023.10 | [LightSeq] LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers(@UC Berkeley etc) |  [[pdf]](https://arxiv.org/pdf/2310.03294.pdf)   |        [[LightSeq]](https://github.com/RulinShao/LightSeq) ![](https://img.shields.io/github/stars/RulinShao/LightSeq.svg?style=social)        |  â­ï¸   |  
| 2023.10 |                                    ğŸ”¥[**TensorRT-LLM**] NVIDIA TensorRT LLM(@NVIDIA)                                    | [[docs]](https://nvidia.github.io/TensorRT-LLM/) |     [[TensorRT-LLM]](https://github.com/NVIDIA/TensorRT-LLM) ![](https://img.shields.io/github/stars/NVIDIA/TensorRT-LLM.svg?style=social)     | â­ï¸â­ï¸  |
| 2023.12 |   ğŸ”¥[**PETALS**] Distributed Inference and Fine-tuning of Large Language Models Over The Internet(@HSE Univesity etc)   |  [[pdf]](https://arxiv.org/pdf/2312.08361.pdf)   | [[petals]](https://github.com/bigscience-workshop/petals) ![](https://img.shields.io/github/stars/bigscience-workshop/petals.svg?style=social) | â­ï¸â­ï¸  |
| 2024.01 | [inferflow]INFERFLOW: AN EFFICIENT AND HIGHLY CONFIGURABLE INFERENCE ENGINE FOR LARGE LANGUAGE MODELS(@Tencent AI Lab)  |  [[pdf]](https://arxiv.org/pdf/2401.08294.pdf)   |      [[inferflow]](https://github.com/inferflow/inferflow) ![](https://img.shields.io/github/stars/inferflow/inferflow.svg?style=social)       |  â­ï¸   |

## äº®ç‚¹/å¯å®æ–½çš„é¡¹ç›®

- **vLLM** ï¼šå¼€ç®±å³ç”¨ï¼Œé›†æˆæœ€æ–°çš„Page Attentionï¼Œæ”¯æŒå•å¡å’Œå¤šå¡æ¨¡å¼ã€‚ä¸Šæ‰‹ç®€å•ï¼Œå¯å¿«é€Ÿå®ç°é«˜æ€§èƒ½æ¨ç†ã€‚
- **DeepSpeed-FastGen/DeepSpeed** ï¼šå¾®è½¯æœ€æ–°æ¡†æ¶ï¼Œé›†æˆäº†æ¨¡å‹çš„è®­ç»ƒã€æ¨ç†å…¨è¿‡ç¨‹ã€‚ç›¸è¾ƒäºå…¶å®ƒä¼˜åŒ–æ¡†æ¶è°ƒèŠ‚éš¾åº¦é«˜ã€æ¨¡å‹ä»£ç å¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒçš„é—®é¢˜ï¼ŒDeepSpeedå¯ä»Pytorchä»£ç ä¸­ç›´æ¥å¼•å…¥ä¼˜åŒ–æ¨¡å—ï¼Œåªéœ€å°‘é‡ä»£ç ä¿®æ”¹å³å¯å®ç°é«˜æ€§èƒ½æ¨ç†ã€‚
æ³¨æ„åœ¨å°è§„æ¨¡æ¨¡å‹çŠ¶å†µä¸‹ï¼ŒDeepSpeedåˆ†å¸ƒå¼çš„è®­ç»ƒå’Œæ¨ç†æ–¹æ³•åœ¨èµ„æºåˆ†é…æ—¶å¯èƒ½æœ‰é¢å¤–çš„æ—¶é—´å¼€é”€ã€‚ç¬”è€…åœ¨å°è¯•è®­ç»ƒå°è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚ç®€å•å…¨è¿æ¥ç¥ç»ç½‘ç»œã€ResNet50ç­‰ï¼‰æ—¶ï¼Œå‘ç°åˆ†é…èµ„æºçš„æ—¶é—´åœ¨æ¡†æ¶è¿è¡Œä¸­æ˜¯éš¾ä»¥å¿½ç•¥çš„ï¼ŒDeepSpeedåè€Œä¼šæœ‰æ›´é•¿çš„æ—¶é—´å¼€é”€ã€‚ 
- **PowerInfer** ï¼šåŸºäºllama.cppçš„åŸºæœ¬æ•°æ®ç»“æ„ï¼Œç‹¬åˆ›å†·çƒ­ç¥ç»å…ƒåˆ†ç±»æŠ€æœ¯ï¼Œæœ‰æ•ˆåˆ†ç¦»ç®—åŠ›éœ€æ±‚è¾ƒé«˜çš„ç¥ç»å…ƒåˆ°æ˜¾å­˜è¿›è¡Œé«˜é€Ÿæ¨ç†ã€å†·ç¥ç»å…ƒå­˜å‚¨åˆ°CPUè¿›è¡Œä½åŠŸè€—æ¨ç†ã€‚é€‚ç”¨äºé…ç½®è¾ƒä½çš„æœåŠ¡å™¨ï¼Œå¦‚éä¸“ä¸šç”¨é€”çš„ä¸ªäººä¸»æœºç­‰ã€‚ä½†ç›®å‰è¿‘æ”¯æŒReLUæ¿€æ´»å‡½æ•°çš„éƒ¨åˆ†æ¨¡å‹ï¼Œä¸”ä¸æ”¯æŒä¸­æ–‡ã€‚

æ³¨æ„åˆ°Tensor-RTå’ŒMegatron-LMä¹Ÿæ˜¯æ¥è‡ªNVIDIAçš„éå¸¸ç«çƒ­çš„æ¡†æ¶ã€‚ä½†ç¬”è€…å°è¯•éƒ¨ç½²è¿™äº›æ¡†æ¶æ—¶ï¼Œå‘ç°å…¶è½¯ä»¶éœ€æ±‚è¾ƒä¸ºå¤æ‚ï¼ˆTensor-RTï¼‰æˆ–åœ¨ä¸€èˆ¬è®¡ç®—èµ„æºä¸‹å¯å®æ–½æ€§è¾ƒä½ï¼ˆMegatron-LMï¼‰ã€‚æ„Ÿå…´è¶£çš„åŒå­¦ä¹Ÿå¯ä»¥äº†è§£ä¸‹ï¼Œæ¯•ç«Ÿæ˜¯æ¥æºäºæœ€å¤§AIç¡¬ä»¶å‚å•†NVIDIAçš„ï¼Œä»–ä»¬çš„å·¥ç¨‹å¸ˆä¼šäº†è§£æ›´æ¥è¿‘ç¡¬ä»¶çš„ä¼˜åŒ–æ–¹æ³•å§ï¼ˆ^*^ï¼‰ã€‚

